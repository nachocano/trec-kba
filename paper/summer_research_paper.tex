\documentclass{article}

\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{mdwlist}

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

%\usepackage{icml2014} 
\usepackage[accepted]{icml2014}


\icmltitlerunning{Streaming Document Classification using Distributed Non-Parametric Representations}

\begin{document} 

\twocolumn[
\icmltitle{Streaming Document Classification using Distributed \\ Non-Parametric Representations}

\icmlauthor{Ignacio Cano}{icano@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}
\icmlauthor{Sameer Singh}{sameer@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}
\icmlauthor{Carlos Guestrin}{guestrin@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}

\icmlkeywords{non-parametric clustering, nlp, word embeddings, vital filtering, streaming}

\vskip 0.3in
]

\begin{abstract} 

Most studies that deal with large corpuses of text documents have focused on identifying references to target entities as well as studying their topics evolution over time. However, current systems are unable to handle streaming data, they do not partition the entity references according to their topics, and they do not identify the references vitalness.
In this paper we introduce a distributed, non-parametric representation of documents that addresses the above limitations. We propose a distributed word embedding representation of entity contexts. Each context is described by topic clusters that are estimated in a non-parametric manner. Further, we associate a novelty measure to each topic cluster, dynamically estimating the relevance of each entity based on document frequencies.
This approach of distributed word embeddings, non-parametric clustering, and novelty measure provides an accurate representation of entity context appropiate for streaming settings that addresses the aforementioned restrictions.


\end{abstract} 

\section{Introduction}
\label{intro}

Filtering streaming documents to accelerate users filling knowledge gaps plays a crucial role in the maintenance and update of knowledge bases.
With the exponential increase of information on the web, it becomes critical to detect relevant documents and incorporate their information to entities in a timely manner \cite{jingang13}.

\cite{frank12} observed a considerable time lag between the publication date of cited articles and the date of the corresponding citations created in Wikipedia. The median time is over a year, and the distribution has a long and heavy tail. This gap could be drastically reduced if automatic systems could suggest relevant documents to editors as soon as they are published.

However, when processing a large corpus of text documents, practitioners are often concerned in finding references to entities of interest \cite{RaoMD10, choi2007}, and studying their topics distributions over time \cite{blei12}. Current tools are somewhat limited; they do not handle online settings, and they do not cluster the entity references according to topics nor identify their importance levels.

Recent studies \cite{xitong13, bouvier13, efron13, zhang13, bellogin13} have centered their attention on solving the above problems with supervised methods, using mainly document, document-entity and temporal level features. By using unlabelled examples to reduce data sparsity in the labeled training data, semi-supervised approaches can improve the generalization accuracy of those supervised systems \cite{Turian10wordrepresentations}.

We therefore introduce a semi-supervised approach suitable for streaming settings that uses a distributed word embedding and a non-parametric topic cluster representation of entity contexts. We also include a novelty measure that approximates the relevance of each entity according to document frequencies. Further, we update the topic identities, number of topics, and the topics novelty in an online fashion, observing only a single document at a time.

This combination of distributed word embeddings, non-parametric clustering, and novelty measure provides an efficient yet accurate representation of entity context that can be updated in a streaming manner, thus addressing the document filtering requirements on large streams of text.

We present experimental results demonstrating the benefits of our method and show that it surpasses previous supervised approaches in TRECKBA14 Vital Filtering task.

\section{Background}
\label{background}

\subsection{Problem Setup}
\label{setup}

We assume a set of $m$ target entities $E = \left\{ {e_1, ..., e_m}\right\}$, e.g. $e_i$ = Barack Obama, and a set of $n$ documents $D = \left\{ {d_1, ..., d_n}\right\}$ that arrive in chronological order, e.g. $d_1$ = ``\dots Obama took an active role in the Senate's drive for improved border security and immigration reform \dots'', $d_2$ = ``Barack Hussein Obama has been elected as President of the United States, he is the first African American to hold the office \dots''.

The task at hand is to predict the truth category of unseen documents in $D$.

We assume an online setting, i.e. the algorithm should provide predictions for documents arriving at time $t$ before seeing documents arriving at time $t+1$.

\subsection{Document Categories}

Documents in $D$ may or may not refer to entities in $E$. In some cases, a candidate mention may have so little context that even a human cannot decide if it refers to the entity or not. Therefore, we use the same document categories stated in TRECKBA14 assessor guidelines

\begin{itemize*}
  \item $Referent$: the document refers to an entity in $E$
    \begin{itemize*}
      \item Vital: the document contains information that at the time it enters the stream, it drives an update to an entity in $E$ with timely, new information about the entity's current state, actions or situation, e.g. ``\dots Barack Obama has been elected President \dots''
      \item Useful: the document contains information that you can use when building an initial profile of an entity in $E$, it means that the document is possibly citable but the information is not timely, e.g. ``\dots Barack Obama was born on August 4th, 1961 \dots''
    \end{itemize*}
  \item $Unknown$: the context is so ambiguous that you cannot decide whether the mention refers to an entity in $E$ or not, e.g. ``\dots Barack is a great father and a better husband \dots''. The mention ``Barack'' may refer to any married parent named Barack.
  \item $Non\mathord{-}referent$: the document doesn't refer to any entity in $E$, e.g. ``\dots Barack Ferrazzano provides a wide range of business-oriented legal \dots''.
\end{itemize*}

\subsection{Word Embeddings}

A word embedding is a dense, low-dimensional, and real-valued vector associated with a word. Each dimension of the embedding represents a latent feature of the word, and hopefully captures useful syntactic and semantic properties \cite{Turian10wordrepresentations}.

The learned vectors computed using neural networks are very attractive because they explicitly encode many linguistic regularities and patterns. Many of these patterns can be represented with simple algebraic operations. For example, the result of $\vec{v_{paris}} - \vec{v_{france}} + \vec{v_{germany}}$ is closer to $\vec{v_{berlin}}$ than to any other word vector \cite{mikolovChen,mikolovYih}.

more stuff

\subsection{Non-parametric Clustering}

Clustering is a ubiquitous task in exploratory data analysis, data mining, and several other application domains. %\cite{KrishnamurthyBXS12}.
Traditionally, the clustering problem is defined in the following way. For a set $X$ of $n$ nodes, a clustering result is a partition $\left\{ {X_1, ..., X_k}\right\}$ of the nodes in $X$ such that $X=\cup_{l=1}^{k}{X_l}$ and $X_r \cap X_s = \emptyset$ for $1 \leq r,s \leq k, r \ne s$ \cite{ChiSZHT07}.

Assuming the $i$-th node in $X$ is a $d$-dimensional vector $\vec{x_i} \in \mathbf{R}^d$, the $k$-means clustering problem is to find a partition $\left\{ {X_1, ..., X_k}\right\}$ that minimizes the following measure
\begin{equation}
\sum_{l=1}^{k} \sum_{i \in X_l} d(\vec{x_i},\vec{\mu_l})
\end{equation}
where $d(\cdot,\cdot)$ is a distance measure and $\vec{\mu_l}$ is the mean vector of the $l$-th cluster, i.e., $\vec{\mu_l} = \sum_{j \in X_l}{\vec{x_j}/|X_l|}$.
In our setting, $d(\cdot,\cdot)$ is the cosine distance
\begin{equation}
d(x,y) = 1 - \cos(x,y) = 1 - \frac{\dot(x,y)}{||x||||y||}
\end{equation}
and $k$ is not fixed beforehand.

We assume an online setting where the clustering algorithm must provide the items partitions at time $t$ before seeing the items at time $t+1$. We further introduce the decision variable used for cluster creation, $\alpha$, an hyperparameter of the model, $0 \leq \alpha \leq 1$.

\subsubsection{Timeless}

As mentioned in \ref{background}, the notion of $vitalness$ is closely related to the timeliness of the new information. We include a timeless feature $\lambda_t$ in every cluster, which is dinamically updated over time, and $0 \leq \lambda_t \leq 1$. Low timeless, i.e. high timeliness, aims to represent vitalness.

The cluster timeless update is a two-step process, an exponential decrease followed by a constant increase.
The timeless decrease is controlled by the hyperparameter $\gamma_d$ and follows an exponential decay model
\begin{equation}
\label{decrease}
\lambda_t = \lambda_{t-1} \exp{(-\gamma_d \frac{(t-(t\mathord{-}1))}{T})}
\end{equation}
where $\lambda_{t-1}$ is the timeless at time $t-1$, $\gamma_d \geq 0$, $t$ is the document's timestamp at time $t$, $t-1$ is the document's timestamp at time $t-1$, and $T$ is a normalizing constant.

On the other hand, the timeless increase is regulated by $\gamma_i$ according to the following expression
\begin{equation}
\lambda_t = 1 - (1 - \lambda_t) \gamma_i
\end{equation}
where $\lambda_t$ on the right side of the equation is computed in \ref{decrease}, and $0 \leq \gamma_i \leq 1$.

The timeless reported at time $t$ is $\lambda_t$ before the increase.

\section{Approach}
\label{approach}

TODO 
We use a streaming clustering algorithm to create entity clusters as follows...

candidate clusters (all, we need efficient methods to select candidate clusters)
similarity metric.

some of sameer things
Each entity context is represented by topic clusters, that are estimated in a non-parametric manner by assuming that the context of each entity in a single document belongs to a single topic. To address the lexical sparsity and generalize to unseen documents, each document is represented by its mean word embedding, while each topic cluster is represented by the mean embedding vector of the documents in the cluster. Further, we associate a timeliness measure to each topic cluster, dynamically estimating the relevance of each entity based on document frequencies. We update the topic identities, number of topics, and the timeliness of topics in an online fashion, observing only a single document at a time. 

picture obama senator, president clusters... evolution over time as a toy example to generate intuition. It may go in section 2 and section 3 contains the formulas... 

\section{Evaluation}
\label{evaluation}

To assess our method we use TRECKBA14 stream corpus. It has around 20M documents annotated with BBN's Serif NLP tools, including within-doc coref and dependency parse trees. Further, we use 71 target entities given by TRECKBA14 organizers for the Vital Filtering task. Among the 20M documents, around 28K have truth labels. We preprocess the corpus to filter the documents that contain exact mentions to the entities names, including canonical and surface form names.

TRECKBA14 Vital Filtering task has the following four document rating levels

\begin{itemize*}
  \item \textbf{Vital} - means that the entity took action or experienced action, e.g. scored a goal, defended his PhD dissertation.
  \item \textbf{Useful} - a document that you might use when building an initial profile, means that the document is possibly citable but not timely, e.g. background bio.
  \item \textbf{Neutral} - means that the entity is mentioned in a confusing context or fragment such that one cannot decide with confidence whether this is the target entity.
  \item \textbf{Garbage} - means that the document contains no information about the target entity, e.g. a surface form name appears and the context confirms that it is a different entity.
\end{itemize*}

We therefore use two tree ensembles classifiers in cascade. The first ensemble distinguishes between $relevant$, i.e. vital and useful, and $non\mathord{-}relevant$, i.e. neutral and garbage, documents, we call it $rnr$. The second classifier further discriminates the $relevant$ documents into vital or useful categories, we call it $uv$.

Both $rnr$ and $uv$ are extremely randomized tree ensembles \cite{GEW06a} with 100 weaker classifiers, each of depth 150.
Every run uses the same $rnr$ model, an extremely randomized tree ensemble, which is trained with the following basic features, borrowed from \cite{jingang13}.

\begin{itemize*}
    \item \textbf{Document Features}
    \begin{itemize*}
        \item $log(length)$: log of document length
        \item $source$: discretized document source
    \end{itemize*}
    \item \textbf{Document-Entity Features}
    \begin{itemize*}
        \item $N(D,E)$: \# of occurrences of the target entity E in document D
        \item $N(D,Ep)$: \# of occurrences of the partial name of target entity E in document D
        \item $FPOS(D,E)$: position of first occurrence of entity E in document D
        \item $FPOSn(D,E)$: $FPOS(D,E)$ normalized by document length
        \item $FPOS(D,Ep)$: position of first occurrence of partial name of entity E in document D
        \item $FPOSn(D,Ep)$: $FPOS(D,Ep)$ normalized by document length
        \item $LPOS(D,E)$: position of last occurrence of entity E in document D
        \item $LPOSn(D,E)$: $LPOS(D,E)$ normalized by document length
        \item $LPOS(D,Ep)$: position of last occurrence of partial name of entity E in document D
        \item $LPOSn(D,Ep)$: $LPOS(D,Ep)$ normalized by document length
        \item $Spread(D,E)$: $LPOS(D,E) - FPOS(D,E)$
        \item $Spreadn(D,E)$: $Spread(D,E)$ normalized by document length
        \item $Spread(D,Ep)$: $LPOS(D,Ep)\mathord{-}FPOS(D,Ep)$
        \item $Spreadn(D,Ep)$: $Spread(D,Ep)$ normalized by document length
    \end{itemize*}
\end{itemize*}

We provide the following $uv$ baselines to compare to our method.
TODO enumerate an explain them

\begin{table}[H]
\center
\begin{tabular}{|c|c|c|} \hline
\textbf{RunId} & \textbf{F1} \\ \hline\hline
$f\_basic\_single$ & 0.355 \\ \hline
$f\_basic\_multi$ & 0.492 \\ \hline
$f\_emb\_comb$ & 0.534 \\ \hline
$f\_emb\_pos$ & 0.498 \\ \hline
$f\_mean\_stat$ & 0.520 \\ \hline
$f\_mean\_dyn$ & 0.524 \\ \hline
$f\_clust\_stat$ & 0.527 \\ \hline
$f\_clust\_dyn$ & 0.518 \\ \hline
\end{tabular}
\caption{TRECKBA14 Vital Filtering scores}
\label{f1}
\end{table}

Table \ref{f1} shows the micro F1 results computed using KBA oficial scorer tool.
short explanation of results.

\section{Related Work}
\label{related}

TODO 
cite paper of sameer's friend.
last year competition papers. result reports.
cite some papers on non parametric evolutionary clustering.

Representing words by dense, real-valued vector embeddings, commonly called distributed representations, helps address the curse of dimensionality
There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. (sameer friend)

One approach that is becoming popular is to use unsupervised methods to induce word features -or to download word features that have already been induced- plug these word features into an existing system, and observe a significant increase in accuracy.

\section{Conclusion}
\label{conclusion}

more...

\section*{Acknowledgments} 
 
\bibliography{summer_research_paper}
\bibliographystyle{icml2014}

\end{document} 


