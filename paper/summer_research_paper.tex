\documentclass{article}

\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure} 

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

%\usepackage{icml2014} 
\usepackage[accepted]{icml2014}


\icmltitlerunning{Streaming Document Filtering using Distributed, Non-Parametric Representations}

\begin{document} 

\twocolumn[
\icmltitle{Streaming Document Filtering using Distributed, Non-Parametric Representations}

\icmlauthor{Ignacio Cano}{icano@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}
\icmlauthor{Sameer Singh}{sameer@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}
\icmlauthor{Carlos Guestrin}{guestrin@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}

\icmlkeywords{non-parametric clustering, nlp, word embeddings, vital filtering, streaming}

\vskip 0.3in
]

\begin{abstract} 

Most studies in filtering large corpuses of text documents have focused on identifying references to target entities as well as studying their prominence and topics evolution over time. However, a major problem with this kind of application appears in streaming settings. Current systems are quite restrictive for this domain; they are unable to handle streaming data, they do not partition the entity references according to their topics, and they do not identify the vitalness of the references.
In this paper we introduce a distributed, non-parametric representation of documents that addresses the above limitations. We propose a distributed word embedding representation of entity contexts. Each context is described by topic clusters that are estimated in a non-parametric manner. Furthermore, we associate a timeless measure to each topic cluster, which dynamically estimates the relevance of the entity based on document frequencies.

\end{abstract} 

\section{Introduction}
\label{intro}

doc classification, vital filtering task in gral. cite some previous year papers...
word embeddings (cite) quick overview.
clustering (cite), non param clustering (cite)
timeliness (see how to put it here)

The rest of this paper is organized as follows. In section \ref{background}, we introduce the notation and the problem setup. In section \ref{approach} we describe our method, and evaluate it in \ref{evaluation} with TRECKBA14 corpus. Section \ref{related} briefly discusses the relevant literature and section \ref{conclusion} lists the conclusions.

\section{Background}
\label{background} 

We assume a set of target entities $E$ and a set of streaming documents $D$. The documents in $D$ arrive in chronological order.

The task at hand is to filter $vital$ documents from $D$ that refer to entities in $E$.
The term $vital$ means that the document contains information that at the time it enters the stream, it would drive an update to the entity's dossier with timely, new information about the entity's current state, actions or situation. The new information must motivate a change to an already up-to-date knowledge base article.

To this end, we first describe the constituent parts of our proposed method.

\subsection{Word Embeddings}

A word embedding is a dense, low-dimensional, and real-valued vector associated with a word. Each dimension of the embedding represents a latent feature of the word, and hopefully captures useful syntactic and semantic properties \cite{Turian10wordrepresentations}.
The learned vectors computed using neural networks are very attractive because they explicitly encode many linguistic regularities and patterns. Many of these patterns can be represented with simple algebraic operations. For example, the result of $\vec{v_{paris}} - \vec{v_{france}} + \vec{v_{germany}}$ is closer to $\vec{v_{berlin}}$ than to any other word vector \cite{mikolovChen,mikolovYih}.

\subsection{Non-parametric Clustering}

Clustering is a ubiquitous task in exploratory data analysis, data mining, and several other application domains. %\cite{KrishnamurthyBXS12}.
Traditionally, the clustering problem is defined in the following way. For a set $X$ of $n$ nodes, a clustering result is a partition $\left\{ {X_1, ..., X_k}\right\}$ of the nodes in $X$ such that $X=\cup_{l=1}^{k}{X_l}$ and $X_r \cap X_s = \emptyset$ for $1 \leq r,s \leq k, r \ne s$ \cite{ChiSZHT07}.

Assuming the $i$-th node in $X$ is a $d$-dimensional vector $\vec{x_i} \in \mathbf{R}^d$, the $k$-means clustering problem is to find a partition $\left\{ {X_1, ..., X_k}\right\}$ that minimizes the following measure
\begin{equation}
\sum_{l=1}^{k} \sum_{i \in X_l} d(\vec{x_i},\vec{\mu_l})
\end{equation}
where $d(\cdot,\cdot)$ is a distance measure and $\vec{\mu_l}$ is the mean vector of the $l$-th cluster, i.e., $\vec{\mu_l} = \sum_{j \in X_l}{\vec{x_j}/|X_l|}$.
In our setting, $d(\cdot,\cdot)$ is the cosine distance
\begin{equation}
d(x,y) = 1 - \cos(x,y) = 1 - \frac{\dot(x,y)}{||x||||y||}
\end{equation}
and $k$ is not fixed beforehand.

We assume an online setting where the clustering algorithm must provide the items' partitions at time $t$ before seeing the items at time $t+1$. We further introduce the decision variable used for cluster creation, $\alpha$, an hyperparameter of the model, $0 \leq \alpha \leq 1$.

\subsubsection{Timeless}

As mentioned in \ref{background}, the notion of $vitalness$ is closely related to the timeliness of the new information. We include a timeless feature $\lambda_t$ in every cluster, which is dinamically updated over time, and $0 \leq \lambda_t \leq 1$. Low timeless, i.e. high timeliness, aims to represent vitalness.

The cluster timeless update is a two-step process, an exponential decrease followed by a constant increase.
The timeless decrease is controlled by the hyperparameter $\gamma_d$ and follows an exponential decay model
\begin{equation}
\label{decrease}
\lambda_t = \lambda_{t-1} \exp{(-\gamma_d \frac{(t-(t\mathord{-}1))}{T})}
\end{equation}
where $\lambda_{t-1}$ is the timeless at time $t-1$, $\gamma_d \geq 0$, $t$ is the document's timestamp at time $t$, $t-1$ is the document's timestamp at time $t-1$, and $T$ is a normalizing constant.

On the other hand, the timeless increase is regulated by $\gamma_i$ according to the following expression
\begin{equation}
\lambda_t = 1 - (1 - \lambda_t) \gamma_i
\end{equation}
where $\lambda_t$ in the right side of the equation is taken from \ref{decrease}, and $0 \leq \gamma_i \leq 1$.

The timeless reported at time $t$ is $\lambda_t$ before the increase.

\section{Approach}
\label{approach}


By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy.
One approach that is becoming popular is to use unsupervised methods to induce word features -or to download word features that have already been induced- plug these word features into an existing system, and observe a significant increase in accuracy.


focus on vital - useful classification.
exponential decay, followed by increase in timeliness.

We use a streaming clustering algorithm to create entity clusters as follows.

candidate clusters (all, we need efficient methods to select candidate clusters)
similarity metric.


\section{Evaluation}
\label{evaluation}

relevant non relevant definitions.
trec kba 2014 filtered corpus.
2 classifiers
around 20M docs in 2M chunks. Filtered based on exact partial matching of entity names.
assessed / unassessed documents.
plots on the different baselines, explanations of all of them.
features table.

\section{Related Work}
\label{related}

last year competition papers. result reports.
cite some papers on non parametric evolutionary clustering.
cite paper of sameer's friend.


\section{Conclusion}
\label{conclusion}

This combination of non-parametric clustering, timeless, and distributed word embeddings provides an efficient yet accurate representation of entity context that can be updated in a streaming manner, thus addressing the requirements from document exploration on large streams of documents.
TODO...

\section*{Acknowledgments} 
 
\bibliography{summer_research_paper}
\bibliographystyle{icml2014}

\end{document} 


