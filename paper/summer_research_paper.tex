\documentclass{article}

\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure} 

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

%\usepackage{icml2014} 
\usepackage[accepted]{icml2014}


\icmltitlerunning{Streaming Document Filtering using Distributed, Non-Parametric Representations}

\begin{document} 

\twocolumn[
\icmltitle{Streaming Document Filtering using Distributed, Non-Parametric Representations}

\icmlauthor{Ignacio Cano}{icano@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}
\icmlauthor{Sameer Singh}{sameer@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}
\icmlauthor{Carlos Guestrin}{guestrin@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}

\icmlkeywords{non-parametric clustering, nlp, word embeddings, vital filtering, streaming}

\vskip 0.3in
]

\begin{abstract} 

Most studies in filtering large corpuses of text documents have focused on identifying references to target entities as well as studying their prominence and topics evolution over time. However, a major problem with this kind of application appears in streaming settings. Current systems are quite restrictive for this domain; they are unable to handle streaming data, they do not partition the entity references according to their topics, and they do not identify the vitalness of the references.
In this paper we introduce a distributed, non-parametric representation of documents that addresses the above limitations. We propose a distributed word embedding representation of entity contexts. Each context is described by topic clusters that are estimated in a non-parametric manner. Furthermore, we associate a timeless measure to each topic cluster, which dynamically estimates the relevance of the entity based on document frequencies.

\end{abstract} 

\section{Introduction}
\label{intro}

doc classification, vital filtering task in gral.
word embeddings (cite)
clustering (cite) 
non param clustering (cite)
timeliness (see how to put it here)

The rest of this paper is organized as follows. In section \ref{background}, we introduce the notation and the problem setup. Then, in section \ref{approach} we review our method, and evaluate it in \ref{evaluation} with TRECKBA14 corpus. 
In section \ref{related}, we briefly discuss the relevant literature.

\section{Background}
\label{background} 

We assume a set of target entities {\tt E} and a set of streaming documents {\tt D}. The documents in {\tt D} arrive in chronological order.

The task at hand is to filter {\it vital} documents from {\tt D} that refer to entities in {\tt E}.
The term {\it vital} means that the document contains information that at the time it enters the stream, it would drive an update to the entity's dossier with timely, new information about the entity's current state, actions or situation. The new information must motivate a change to an already up-to-date knowledge base article.

To this end, we first describe the constituent parts of our proposed method.

\subsection{Word Embeddings}

A word embedding is a dense, low-dimensional, and real-valued vector associated with a word. Each dimension of the embedding represents a latent feature of the word, and hopefully captures useful syntactic and semantic properties \cite{Turian10wordrepresentations}.
The learned vectors computed using neural networks are very attractive because they explicitly encode many linguistic regularities and patterns. Many of these patterns can be represented with simple algebraic operations. For example, the result of $\vec{v_{paris}} - \vec{v_{france}} + \vec{v_{germany}}$ is closer to $\vec{v_{berlin}}$ than to any other word vector \cite{mikolovChen,mikolovYih}.

\subsection{Non-parametric Clustering}

Clustering is a ubiquitous task in exploratory data analysis, data mining, and several other application domains \cite{KrishnamurthyBXS12}.
We assume an online setting where the clustering algorithm must provide the items' partitions at time {\it t} before seeing the items at time {\it t+1}.
We assume each document corresponds to a single cluster. In order to assign documents to clusters, we introduce the hyperparameter $\alpha$.

\subsubsection{Timeless}

As mentioned in \ref{background}, the notion of {\it vitality} has to do with the timeliness of the new information. We introduce a timeless feature in every cluster. It follows the aging method, where the clusters are associated with weights that decrease over time, according to an exponential decay model (cite)

In most applications very old data points are considered less useful and relevant than more recent data. Thus it is necessary that old or stale data does not overly influence the statistics or models that are built from the analysis. There are two common approaches to deal with the problem of staleness of historical data. One method is aging, data elements are associated with weigths that decrease over time. In most algorithms that use aging, the weights decrease according to the computationally-simple exponential decay model. ()


problem, document filtering in an online fashion, 
relevant non relevant definitions.
vital useful definitions.
clustering loss function, alpha, gamma, what they mean.
timeliness (maybe timeless is better to use)

\section{Approach}
\label{approach}


By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy.
One approach that is becoming popular is to use unsupervised methods to induce word features -or to download word features that have already been induced- plug these word features into an existing system, and observe a significant increase in accuracy.


focus on vital - useful classification.
exponential decay, followed by increase in timeliness.

We use a streaming clustering algorithm to create entity clusters as follows.

candidate clusters (all, we need efficient methods to select candidate clusters)
similarity metric.


\section{Evaluation}
\label{evaluation}

trec kba 2014 filtered corpus.
around 20M docs in 2M chunks. Filtered based on exact partial matching of entity names.
assessed / unassessed documents.
plots on the different baselines, explanations of all of them.
features table.

\section{Related Work}
\label{related}

last year competition papers. result reports.
cite some papers on non parametric evolutionary clustering.
cite paper of sameer's friend.


\section{Conclusion}

This combination of non-parametric clustering, timeless, and distributed word embeddings provides an efficient yet accurate representation of entity context that can be updated in a streaming manner, thus addressing the requirements from document exploration on large streams of documents.
TODO...

\section*{Acknowledgments} 
 
\bibliography{summer_research_paper}
\bibliographystyle{icml2014}

\end{document} 
