\documentclass{article}

\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{mdwlist}
\usepackage{bbm}

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

%\usepackage{icml2014} 
\usepackage[accepted]{icml2014}


\icmltitlerunning{Streaming Document Classification using Distributed Non-Parametric Representations}

\begin{document} 

\twocolumn[
\icmltitle{Streaming Document Classification using Distributed \\ Non-Parametric Representations}

\icmlauthor{Ignacio Cano}{icano@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}
\icmlauthor{Sameer Singh}{sameer@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}
\icmlauthor{Carlos Guestrin}{guestrin@cs.washington.edu}
\icmladdress{University of Washington, P. Allen Center, 185 Stevens Way, Seattle, WA 98195 USA}

\icmlkeywords{non-parametric clustering, nlp, word embeddings, vital filtering, streaming}

\vskip 0.3in
]

\begin{abstract} 

Most studies that deal with large corpuses of text documents have focused on identifying references to target entities as well as studying their topics evolution over time. However, current systems are unable to handle streaming data, they do not partition the entity references according to their topics, and they do not identify the references vitalness.
In this paper we introduce a distributed, non-parametric representation of documents that addresses the above limitations. We propose a distributed word embedding representation of entity contexts. Each context is described by topic clusters that are estimated in a non-parametric manner. Further, we associate a novelty measure to each entity and topic cluster, dynamically estimating the relevance of each entity and topic cluster based on document frequencies.
This approach of distributed word embeddings, non-parametric clustering, and novelty measure, provides an accurate representation of entity contexts appropiate for streaming settings, while addressing the aforementioned restrictions.


\end{abstract} 

\section{Introduction}
\label{intro}

Filtering streaming documents to accelerate users filling knowledge gaps plays a crucial role in the maintenance and update of knowledge bases.
With the exponential increase of information on the web, it becomes critical to detect relevant documents and incorporate their information to entities in a timely manner \cite{jingang13}.

\cite{frank12} observed a considerable time lag between the publication date of cited articles and the date of the corresponding citations created in Wikipedia. The median time is over a year, and the distribution has a long and heavy tail. This gap could be drastically reduced if automatic systems could suggest relevant documents to editors as soon as they are published.

However, when processing a large corpus of text documents, practitioners are often concerned in finding references to entities of interest \cite{RaoMD10, choi2007}, and studying their topics distributions over time \cite{blei12}. Current tools are somewhat limited; they do not handle online settings, and they do not cluster entity references according to topics nor identify the references importance.

Recent studies \cite{xitong13, bouvier13, efron13, zhang13, bellogin13} have centered their attention on solving the above problems with supervised methods, using mainly document, document-entity and temporal level features. \cite{Turian10wordrepresentations} showed that by using unlabelled examples to reduce data sparsity in the labeled training data, semi-supervised approaches can improve the generalization accuracy of those supervised systems.

We therefore introduce a semi-supervised approach suitable for streaming settings that uses a distributed word embedding and a non-parametric topic cluster representation of entity contexts. We also include a novelty measure that approximates the relevance of each entity and its topic clusters according to document frequencies. Further, we update the topic identities, number of topics, and the entities and topics novelties in an online fashion, observing only a single document at a time.

This combination of distributed word embeddings, non-parametric clustering, and novelty measure provides an efficient yet accurate representation of entity context that can be updated in a streaming manner, thus addressing the document filtering requirements on large streams of text.

We present experimental results demonstrating the benefits of our method and show that it surpasses previous supervised approaches in TRECKBA14 Vital Filtering task.

\section{Background}
\label{background}

\subsection{Problem Setup}
\label{setup}

We assume a set of $m$ target entities $E = \left\{ {e_1, ..., e_m}\right\}$, e.g. $e_1$ = Barack Obama, and a set of $n$ documents $D = \left\{ {d_1, ..., d_n}\right\}$ that arrive in chronological order, e.g. $d_1$ = ``\dots Obama took an active role in the Senate's drive for improved border security and immigration reform \dots'', $d_2$ = ``Barack Obama has been elected as President of the United States, he is the first African American to hold the office \dots'', $d_3$ = ``Obama is the 44th and current President of the United States \dots''.

The task at hand is to predict the truth category of unseen documents in $D$.

We assume an online setting, i.e. the algorithm should provide predictions for documents arriving at time $t$ before seeing documents arriving at time $t+1$.

\subsection{Document Categories}
\label{categories}

Documents in $D$ may or may not refer to entities in $E$. In some cases, a candidate mention may have so little context that even a human cannot decide if it refers to the entity or not. Therefore, we use the same document categories stated in TRECKBA14 assessor guidelines.

\begin{itemize*}
  \item $Referent$: the document refers to an entity in $E$
    \begin{itemize*}
      \item Vital: the document contains information that at the time it enters the stream, it drives an update to an entity in $E$ with timely, new information about the entity's current state, actions or situation, e.g. ``\dots Barack Obama has been elected President \dots''.
      \item Useful: the document contains information that can be used when building an initial profile of an entity in $E$, it means that the document is possibly citable but the information is not timely, e.g. ``\dots Barack Obama was born on August 4th, 1961 \dots''.
    \end{itemize*}
  \item $Unknown$: the context is so ambiguous that you cannot decide whether the mention refers to an entity in $E$ or not, e.g. ``\dots Barack is a great father and a better husband \dots''. The mention ``Barack'' may refer to any married parent named Barack.
  \item $Non\mathord{-}referent$: the document doesn't refer to any entity in $E$, e.g. ``\dots Barack Ferrazzano provides a wide range of business-oriented legal \dots''.
\end{itemize*}

\subsection{Word Embeddings}

A word embedding is a dense, low-dimensional, and real-valued vector associated with a word. Each dimension of the embedding represents a latent feature of the word, and hopefully captures useful syntactic and semantic properties \cite{Turian10wordrepresentations}.

The learned vectors computed using neural networks are very attractive because they explicitly encode many linguistic regularities and patterns. Many of these patterns can be represented with simple algebraic operations. For example, the result of $\vec{v_{paris}} - \vec{v_{france}} + \vec{v_{germany}}$ is closer to $\vec{v_{berlin}}$ than to any other word vector \cite{mikolovChen,mikolovYih}.

Given recent methods for their fast estimation at very large scale, there is rising interest in vector-space word embeddings and their use in NLP \cite{Arvind14}.

\section{Method}
\label{approach}

\subsection{Document Word Embedding}
\label{docwordemb}

We use word embeddings to represent documents. To address the lexical sparsity and generalize to unseen documents, each document in $D$ is represented by its mean word embedding vector computed from lemmas that appear in sentences mentioning entities in $E$. The process is shown in Algorithm \ref{wordembedding}. 

As an example, let's suppose we extract the following lemmas from document $d_2$ for entity $e_1$ shown in section \ref{setup}, conditioning on nouns and verbs: [barack obama, elected, president, united states, first, african american, hold, office]. For every lemma extracted, we then compute its word embedding and add them up in an aggregate vector. We finally take the mean of the aggregate vector, which constitutes the distributed word embedding representation of the document.

\begin{algorithm}[tb]
   \caption{Document Word Embedding}
   \label{wordembedding}
\begin{algorithmic}
   \STATE {\bfseries Input:} document $d$, entity $e$, condition $c$
   \STATE {\bfseries Output:} document word embedding mean vector
   \STATE {\bfseries Body:}
   \STATE Initialize $aggregate = \vec{0}$
   \STATE $lemmas = extract(d, e, c)$
   \FOR{$lemma$ {\bfseries in} $lemmas$}
   \STATE $aggregate += word\_embedding(lemma)$
   \ENDFOR
   \STATE $aggregate /= len(lemmas)$
   \STATE return $aggregate$
\end{algorithmic}
\end{algorithm}


\subsection{Non-parametric Clustering}

Each entity context is represented by topic clusters, which are estimated in a non-parametric manner by assuming that the context of each entity in a single document belongs to a single topic. As mentioned in section \ref{docwordemb}, each document is represented by its mean word embedding. Here, each topic cluster of an entity context is represented by the mean embedding vector of the documents in that cluster.

Our approach is closely related to the online non-parametric clustering procedure described in \cite{Arvind14}.
We use Algorithm \ref{nonparamclustering} to create topic clusters for entity contexts.

The number of topic clusters for an entity context is unkown beforehand. Initially, entity contexts do not have topic clusters. We create the first topic cluster for an entity context on its first occurrence in the training data. After creating the first topic cluster for an entity context, a new topic cluster is created when the cosine distance between the word embedding representation of the new arriving document with every topic cluster center of the same entity is greater than or equal to $\alpha$, where $\alpha$ is an hyperparameter of the model, and $0 \leq \alpha \leq 1$. In case the distance is less than $\alpha$, we add the new document to the closest topic cluster and update its center.

\begin{algorithm}[tb]
   \caption{Non-parametric Clustering}
   \label{nonparamclustering}
\begin{algorithmic}
   \STATE {\bfseries Input:} $doc=doc\_word\_embedding(d, e, c)$, and topic clusters list $tcs$ for entity $e$
   \STATE {\bfseries Output:} new or updated topic cluster $tc$
   \STATE {\bfseries Body:}
   \STATE Initialize $tc = nil$
   \IF{$tcs$ {\bfseries is empty}}
    \STATE $tc = create\_topic\_cluster(center\mathord{=}doc)$
    \STATE $tcs.append(tc)$
   \ELSE
     \STATE $dist, i \mathord{=} \min_{\forall{i \in tcs}}{cosine\_dist(tcs[i].center, doc)}$
     \IF{$dist >= \alpha$ }
        \STATE $tc = create\_topic\_cluster(center\mathord{=}doc)$
        \STATE $tcs.append(tc)$
     \ELSE
        \STATE $tc = update\_topic\_cluster(tcs[i], doc)$
     \ENDIF
   \ENDIF
   \STATE return $tc$
\end{algorithmic}
\end{algorithm}

As an example, let's assume we see the documents mentioned in section \ref{setup}. Document $d1$ appears first, it talks about the days Barack Obama was a Senator. As is the first document refering to the entity Obama, we create a new topic cluster $senator$ and add $d1$ to it. Then, document $d2$ appears in the stream. It refers to Obama as being elected President of the United States. The distance with the previous cluster $senator$ is greater than $\alpha$, therefore the algorithm proceeds to create a new topic cluster $president$ and adds $d2$ to it. Finally, $d3$ enters the stream. It talks about Obama as the current President of the U.S. The algorithm compares its distance to the previous clusters and finds that is closest to the $president$ cluster. The distance is less than $\alpha$, hence it adds $d3$ to the $president$ cluster and updates its center.

%\begin{figure}[h!]
%\centering
%\includegraphics[width=.5\textwidth]{kmeans.png}
%\label{clusteringFig}
%\end{figure}

\subsection{Novelty}
\label{novelty}

As mentioned in section \ref{categories}, the notion of $vitalness$ is closely related to the timeliness of the new information. We include a novelty feature $\lambda_i$, which is dinamically updated over time, and $0 \leq \lambda_i \leq 1$. High novelty aims to represent vital documents.

This novelty measure can be used both for entities and topic clusters.

The novelty update is a two-step process, an exponential decrease followed by a constant increase.
The novelty decrease is controlled by the hyperparameter $\gamma_d$ and follows an exponential decay model
\begin{equation}
\label{decrease}
\lambda_i = \lambda_{i-1} \exp{(-\gamma_d \frac{t_i-t_{i-1}}{T})}
\end{equation}
where $\gamma_d \geq 0$, $t_i$ is the $i$-th document timestamp, $t_{i-1}$ is the $i-1$ document timestamp, and $T$ is a normalizing constant.
In case we use novelty as an entity measure, $\lambda_{i-1}$ alludes to the previous document in the stream that refers to the same entity as document $i$.
If we use novelty as a topic cluster measure, $\lambda_{i-1}$ refers to the previous document that belongs to the same topic cluster as document $i$.

On the other hand, the novelty increase is regulated by $\gamma_i$ according to the following expression
\begin{equation}
\lambda_i = 1 - (1 - \lambda_i) \gamma_i
\end{equation}
where $\lambda_i$ on the right side is computed in equation \ref{decrease}, and $0 \leq \gamma_i \leq 1$.

The $i$-th novelty reported is $\lambda_i$ before the increase.


\section{Evaluation}
\label{evaluation}

\subsection{Data}

To assess our method we use TRECKBA14 stream corpus. It has around 20M documents annotated with BBN's Serif NLP tools, including within-doc coref and dependency parse trees. Further, we use the 71 target entities given by TRECKBA14 organizers for the Vital Filtering task. Among the 20M documents, around 28K have truth labels. From these, only 8K are training instances while the rest are test examples.

\subsubsection{Preprocess}

We preprocess the corpus to filter the documents that contain exact string matches to the target entities names, including canonical and surface form names.

\subsection{Classifiers}

We use two extremely randomized tree ensembles classifiers \cite{GEW06a} in cascade, each composed of 100 weak learners. Each tree in the ensembles has depth 150.

\begin{itemize*}
    \item $rnr$: classifies between $referent$ and $non\mathord{-}referent$ or $unkown$ documents.
    \item $uv$: discriminates the $referent$ documents into vital or useful categories.
\end{itemize*}

Each document goes through the $rnr$ classifier. The $referent$ outputs from $rnr$ are used as input to the $uv$ ensemble.

\subsection{Features}
\label{feat}

A summary of the features used in this paper is presented below. Basic features are borrowed from \cite{jingang13}.

\begin{itemize*}
  \item \textbf{Basic}
    \begin{itemize*}
      \item $Document$
        \begin{itemize*}
            \item $log(length)$: log of document length
            \item $source$: discretized document source
        \end{itemize*}
      \item $Document-Entity$
        \begin{itemize*}
            \item $n(d,e)$: \# of occurrences of the target entity $e$ in document $d$
            \item $n(d,e_p)$: \# of occurrences of partial name of target entity $e$ in document $d$
            \item $fpos(d,e)$: position of first occurrence of entity $e$ in document $d$
            \item $fpos_n(d,e)$: $fpos(d,e)$ normalized by document length
            \item $fpos(d,e_p)$: position of first occurrence of partial name of entity $e$ in document $d$
            \item $fpos_n(d,e_p)$: $fpos(d,e_p)$ normalized by document length
            \item $lpos(d,e)$: position of last occurrence of entity $e$ in document $d$
            \item $lpos_n(d,e)$: $lpos(d,e)$ normalized by document length
            \item $lpos(d,e_p)$: position of last occurrence of partial name of entity $e$ in document $d$
            \item $lpos_n(d,e_p)$: $lpos(d,e_p)$ normalized by document length
            \item $spread(d,e)$: $lpos(d,e) - fpos(d,e)$
            \item $spread_n(d,e)$: $spread(d,e)$ normalized by document length
            \item $spread(d,e_p)$: $lpos(d,e_p)\mathord{-}fpos(d,ep)$
            \item $spread_n(d,e_p)$: $spread(d,e_p)$ normalized by document length
        \end{itemize*}
    \end{itemize*}
  \item \textbf{Embeddings}
    \begin{itemize*}
        \item $\vec{\mu(pos)}$: mean word embedding of certain part of speech tag in the entity context
    \end{itemize*}
  \item \textbf{Non-parametric Clustering}
    \begin{itemize*}
        \item $min$: minimum distance to existent topic clusters
        \item $avg$: avg distance to existent topic clusters
        \item $zero$: $\mathbbm{1}_{\vec{0}}{(doc)}$, i.e. flag set to 1 if the word embedding representation of the document is $\vec{0}$
    \end{itemize*}
  \item \textbf{Novelty}
    \begin{itemize*}
        \item $novelty(e)$: novelty of entity $e$, see section \ref{novelty}
        \item $novelty(tc)$: novelty of topic cluster $tc$.
    \end{itemize*}
\end{itemize*}


\subsection{Experiments}

We perform 8 experiments. All of them use the same $rnr$ model trained with the basic features listed in section \ref{feat}.

The different methods differ only on the $uv$ classifier. Explanations of the different $uv$ configurations are detailed below.

\begin{itemize*}
  \item $f\_basic\_single$: baseline method. Uses Basic $Document$ and $Document-Entity$ features.
  \item $f\_basic\_multi$: baseline method. Same as $f\_basic\_single$ but with multitask-learning \cite{Caruana93multitasklearning}.
  \item $f\_emb\_comb$: same as $f\_basic\_multi$ but with Embeddings features computed using the pre-trained Google News dataset. A single combined embedding is calculated from the nouns, proper nouns and verbs found in the entities contexts.
  \item $f\_emb\_pos$: same as $f\_emb\_comb$ but instead of a single combined embedding, it includes one embedding per word type, i.e. one for nouns, one for proper nouns, and one for verbs.
  \item $f\_mean\_stat$: same as $f\_emb\_pos$ but with Novelty and Non-parametric Clustering features, computed per word type. $\alpha = 1$, $\gamma_d = 0$, $\gamma_i = 1$.
  \item $f\_mean\_dyn$: same as $f\_mean\_stat$ but with different hyperparameters. $\alpha = 1$, $\gamma_d = 1$, $\gamma_i = 0.1$.
  \item $f\_clust\_stat$: same as $f\_mean\_stat$ but with different hyperparameters. $\alpha = 0.8$, $\gamma_d = 0$, $\gamma_i = 1$.
  \item $f\_clust\_dyn$: same as $f\_mean\_stat$ but with different hyperparameters. $\alpha = 0.8$, $\gamma_d = 1$, $\gamma_i = 0.1$.
\end{itemize*}

\subsection{Results}

Table \ref{res} shows the micro F1 and SU results of our methods, computed using KBA oficial scorer tool.

\begin{table}[H]
\center
\begin{tabular}{|c|c|c|c|c|} \hline
\textbf{Model} & \textbf{F1} & \textbf{SU} \\ \hline\hline
$f\_basic\_single$ & 0.355 & 0.437 \\ \hline
$f\_basic\_multi$ & 0.492 & 0.520 \\ \hline
$f\_emb\_comb$ & 0.534 & 0.537 \\ \hline
$f\_emb\_pos$ & 0.498 & 0.518 \\ \hline
$f\_mean\_stat$ & 0.520 & 0.532 \\ \hline
$f\_mean\_dyn$ & 0.524 & 0.535 \\ \hline
$f\_clust\_stat$ & 0.527 & 0.537 \\ \hline
$f\_clust\_dyn$ & 0.518 & 0.529 \\ \hline
\end{tabular}
\caption{TRECKBA14 Vital Filtering scores}
\label{res}
\end{table}

As reported in Table \ref{res}, $f\_emb\_comb$ approach is the overall best run on the $vital$ filtering task. Our two baselines ($f\_basic\_single$ and $f\_basic\_multi$) perform as expected. The F1 difference between the two baselines evidences that multitask-learning does work.

All the more advanced runs perform better than both baselines. Using a combined embedding ($f\_emb\_comb$) outperforms using individual embeddings per part of speech tags ($f\_emb\_pos$).

Though the non-parametric clustering and novelty runs perform slightly worse than the simple $f\_emb\_comb$ approach, they illustrate the importance of these new features as they improve the performance of the $f\_emb\_pos$ model.

The worse performance of these more advanced models may be caused by the uncertainty of enough training data and the lack of hyperparameter tunning. We suspect that learning the right $\alpha$ could improve the overall performance of the system.

\section{Related Work}
\label{related}

TODO 
cite paper of sameer's friend.
last year competition papers. result reports.
cite some papers on non parametric evolutionary clustering.

Representing words by dense, real-valued vector embeddings, commonly called distributed representations, helps address the curse of dimensionality
There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. (sameer friend)

One approach that is becoming popular is to use unsupervised methods to induce word features -or to download word features that have already been induced- plug these word features into an existing system, and observe a significant increase in accuracy.

\section{Conclusion}
\label{conclusion}

In this paper we introduce a semi-supervised learning model for document classification tasks. We propose a distributed, non-parametric representation of documents suitable for streaming settings, that groups entity references into topic clusters. Further, we present a notion of novelty computed per entity as well as per topic cluster, which dynamically estimates the entity and cluster relevances.

Combining these three core ideas, distributed word embeddings, non-parametric clustering, and novelty, results in a more accurate representation of entity contexts, and simultaneously addresses the filtering requirements of large corpuses of streaming text documents.

A possible line of future reseach would be exploring hierarchical clustering algorithms to better represent topic clusters. It would also be interesting to assess the effects of using different pre-trained word embeddings. Finally, further experimental investigations are needed to account for the correct tunning of the hyperparameters of the model.

\section*{Acknowledgments} 
 
This work was supported in part by the Argentine Ministry of Science, Technology and Productive Innovation and the TerraSwarm Research Center, supported by the STARnet phase of the Focus Center Research Program (FCRP). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.

\bibliography{summer_research_paper}
\bibliographystyle{icml2014}

\end{document} 


